{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üñ•Ô∏è System and GPU Benchmark\n",
    "\n",
    "A minimal benchmark to test your system's CPU and GPU performance, with improved detection for dual-GPU laptops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports for system information\n",
    "import os\n",
    "import sys\n",
    "import platform\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "import timeit\n",
    "\n",
    "# Advanced PyTorch Benchmark with robust visualization\n",
    "import torch\n",
    "from torch.utils.benchmark import Timer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import PyTorch for benchmarking\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set seaborn style for prettier plots\n",
    "sns.set_style(\"darkgrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up better aesthetics\n",
    "sns.set_theme(style=\"whitegrid\", context=\"notebook\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 8)\n",
    "plt.rcParams[\"font.family\"] = \"sans-serif\"\n",
    "plt.rcParams[\"font.sans-serif\"] = [\n",
    "    \"Arial\",\n",
    "    \"DejaVu Sans\",\n",
    "    \"Liberation Sans\",\n",
    "    \"Bitstream Vera Sans\",\n",
    "    \"sans-serif\",\n",
    "]\n",
    "\n",
    "# Custom color palette\n",
    "colors = {\n",
    "    \"cpu\": \"#3498db\",  # Blue\n",
    "    \"gpu\": \"#e74c3c\",  # Red\n",
    "    \"speedup\": \"#2ecc71\",  # Green\n",
    "    \"background\": \"#f9f9f9\",\n",
    "    \"text\": \"#2c3e50\",\n",
    "    \"grid\": \"#ecf0f1\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã System Information\n",
    "\n",
    "Let's gather basic information about your system hardware and Python environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_header(title):\n",
    "    \"\"\"Print a section header\"\"\"\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"{title.center(80)}\")\n",
    "    print(f\"{'=' * 80}\\n\")\n",
    "\n",
    "\n",
    "def print_info(label, info):\n",
    "    \"\"\"Print a formatted label and info\"\"\"\n",
    "    print(f\"{label.ljust(25)}: {info}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "                               SYSTEM INFORMATION                               \n",
      "================================================================================\n",
      "\n",
      "OS                       : Linux 6.12.35\n",
      "OS Version               : #1-NixOS SMP PREEMPT_DYNAMIC Fri Jun 27 10:11:46 UTC 2025\n",
      "Machine                  : x86_64\n",
      "Processor                : \n",
      "CPU Count (logical)      : 16\n",
      "Python Version           : 3.12.11\n",
      "Python Executable        : /home/yrrrrrf/Documents/Lab/ai/.venv/bin/python\n",
      "PyTorch Version          : 2.6.0+cu124\n",
      "System RAM               : 30.57 GiB\n"
     ]
    }
   ],
   "source": [
    "print_header(\"SYSTEM INFORMATION\")\n",
    "\n",
    "# OS and platform information\n",
    "uname = platform.uname()\n",
    "print_info(\"OS\", f\"{uname.system} {uname.release}\")\n",
    "print_info(\"OS Version\", uname.version)\n",
    "print_info(\"Machine\", uname.machine)\n",
    "\n",
    "# CPU information\n",
    "print_info(\"Processor\", uname.processor)\n",
    "print_info(\"CPU Count (logical)\", os.cpu_count())\n",
    "\n",
    "# Try to get better CPU info on Windows\n",
    "try:\n",
    "    if platform.system() == \"Windows\":\n",
    "        import ctypes\n",
    "\n",
    "        ctypes.windll.kernel32.GetSystemFirmwareTable.restype = ctypes.c_void_p\n",
    "        result = (\n",
    "            subprocess.check_output(\"wmic cpu get name\", shell=True)\n",
    "            .decode()\n",
    "            .strip()\n",
    "            .split(\"\\n\")[1]\n",
    "        )\n",
    "        print_info(\"CPU Model\", result)\n",
    "except Exception as e:\n",
    "    pass  # Fall back to the basic information if this fails\n",
    "\n",
    "# Python information\n",
    "print_info(\n",
    "    \"Python Version\",\n",
    "    f\"{sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}\",\n",
    ")\n",
    "print_info(\"Python Executable\", sys.executable)\n",
    "print_info(\"PyTorch Version\", torch.__version__)\n",
    "\n",
    "# Memory information\n",
    "try:\n",
    "    if platform.system() == \"Windows\":\n",
    "        import ctypes\n",
    "\n",
    "        kernel32 = ctypes.windll.kernel32\n",
    "        c_ulonglong = ctypes.c_ulonglong\n",
    "\n",
    "        class MEMORYSTATUSEX(ctypes.Structure):\n",
    "            _fields_ = [\n",
    "                (\"dwLength\", ctypes.c_ulong),\n",
    "                (\"dwMemoryLoad\", ctypes.c_ulong),\n",
    "                (\"ullTotalPhys\", c_ulonglong),\n",
    "                (\"ullAvailPhys\", c_ulonglong),\n",
    "                (\"ullTotalPageFile\", c_ulonglong),\n",
    "                (\"ullAvailPageFile\", c_ulonglong),\n",
    "                (\"ullTotalVirtual\", c_ulonglong),\n",
    "                (\"ullAvailVirtual\", c_ulonglong),\n",
    "                (\"ullExtendedVirtual\", c_ulonglong),\n",
    "            ]\n",
    "\n",
    "        memoryStatus = MEMORYSTATUSEX()\n",
    "        memoryStatus.dwLength = ctypes.sizeof(MEMORYSTATUSEX)\n",
    "        kernel32.GlobalMemoryStatusEx(ctypes.byref(memoryStatus))\n",
    "        mem_gib = memoryStatus.ullTotalPhys / (1024.0**3)\n",
    "        print_info(\"System RAM\", f\"{mem_gib:.2f} GiB\")\n",
    "        print_info(\"Memory Usage\", f\"{memoryStatus.dwMemoryLoad}%\")\n",
    "    elif (\n",
    "        hasattr(os, \"sysconf\")\n",
    "        and os.sysconf_names.get(\"SC_PHYS_PAGES\", None) is not None\n",
    "    ):\n",
    "        # Linux/Unix\n",
    "        mem_bytes = os.sysconf(\"SC_PHYS_PAGES\") * os.sysconf(\"SC_PAGE_SIZE\")\n",
    "        mem_gib = mem_bytes / (1024.0**3)\n",
    "        print_info(\"System RAM\", f\"{mem_gib:.2f} GiB\")\n",
    "except Exception as e:\n",
    "    print_info(\"System RAM\", \"Could not detect\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç GPU Information\n",
    "\n",
    "Let's check for available GPUs using multiple detection methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "                                GPU INFORMATION                                 \n",
      "================================================================================\n",
      "\n",
      "Method 1: PyTorch CUDA detection\n",
      "CUDA Available           : False\n",
      "CUDA Version             : 12.4\n",
      "PyTorch CUDA detection: No CUDA-compatible GPU detected.\n",
      "\n",
      "Method 3: Manual CUDA initialization attempt\n",
      "Manual CUDA initialization error: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx\n",
      "\n",
      "Final GPU Status:\n",
      "‚úó No CUDA-capable GPU detected or accessible\n"
     ]
    }
   ],
   "source": [
    "print_header(\"GPU INFORMATION\")\n",
    "\n",
    "# Method 1: PyTorch CUDA detection\n",
    "print(\"Method 1: PyTorch CUDA detection\")\n",
    "print_info(\"CUDA Available\", torch.cuda.is_available())\n",
    "print_info(\n",
    "    \"CUDA Version\",\n",
    "    torch.version.cuda if hasattr(torch.version, \"cuda\") else \"Not detected\",\n",
    ")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print_info(\"GPU Count\", torch.cuda.device_count())\n",
    "\n",
    "    # Get information for each GPU\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"\\nGPU {i}:\")\n",
    "        print_info(\"  Name\", torch.cuda.get_device_name(i))\n",
    "        print_info(\n",
    "            \"  Capability\",\n",
    "            f\"{torch.cuda.get_device_capability(i)[0]}.{torch.cuda.get_device_capability(i)[1]}\",\n",
    "        )\n",
    "\n",
    "        # Get memory information if available\n",
    "        try:\n",
    "            total_mem = torch.cuda.get_device_properties(i).total_memory / (1024.0**3)\n",
    "            print_info(\"  Memory\", f\"{total_mem:.2f} GiB\")\n",
    "        except:\n",
    "            pass\n",
    "else:\n",
    "    print(\"PyTorch CUDA detection: No CUDA-compatible GPU detected.\")\n",
    "\n",
    "# Method 2: Try to detect GPUs using Windows commands on Windows systems\n",
    "if platform.system() == \"Windows\":\n",
    "    print(\"\\nMethod 2: Windows command-line detection\")\n",
    "    try:\n",
    "        # Using wmic to detect NVIDIA GPUs\n",
    "        nvidia_output = (\n",
    "            subprocess.check_output(\n",
    "                \"wmic path win32_VideoController get name\", shell=True\n",
    "            )\n",
    "            .decode()\n",
    "            .strip()\n",
    "            .split(\"\\n\")\n",
    "        )\n",
    "        nvidia_output = [\n",
    "            line.strip()\n",
    "            for line in nvidia_output\n",
    "            if line.strip() and line.strip() != \"Name\"\n",
    "        ]\n",
    "\n",
    "        if nvidia_output:\n",
    "            print(\"\\nDetected Graphics Cards:\")\n",
    "            for i, gpu in enumerate(nvidia_output):\n",
    "                print_info(f\"GPU {i}\", gpu)\n",
    "\n",
    "            # Look for NVIDIA GPUs that might work with CUDA\n",
    "            nvidia_gpus = [\n",
    "                gpu\n",
    "                for gpu in nvidia_output\n",
    "                if \"NVIDIA\" in gpu\n",
    "                or \"GeForce\" in gpu\n",
    "                or \"Quadro\" in gpu\n",
    "                or \"RTX\" in gpu\n",
    "                or \"GTX\" in gpu\n",
    "            ]\n",
    "            amd_gpus = [gpu for gpu in nvidia_output if \"AMD\" in gpu or \"Radeon\" in gpu]\n",
    "\n",
    "            if nvidia_gpus:\n",
    "                print(\"\\nDetected NVIDIA GPUs (potentially CUDA-compatible):\")\n",
    "                for i, gpu in enumerate(nvidia_gpus):\n",
    "                    print_info(f\"NVIDIA GPU {i}\", gpu)\n",
    "\n",
    "            if amd_gpus:\n",
    "                print(\"\\nDetected AMD GPUs:\")\n",
    "                for i, gpu in enumerate(amd_gpus):\n",
    "                    print_info(f\"AMD GPU {i}\", gpu)\n",
    "        else:\n",
    "            print(\"No GPUs detected via Windows command line.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error detecting GPUs via Windows command line: {e}\")\n",
    "\n",
    "# Method 3: Try a more direct approach with CUDA initialization (if not already detected)\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"\\nMethod 3: Manual CUDA initialization attempt\")\n",
    "    try:\n",
    "        # Try to explicitly initialize CUDA\n",
    "        if torch.version.cuda is not None:\n",
    "            torch.cuda.init()\n",
    "            print(\"Manual CUDA initialization: Success\")\n",
    "            print_info(\"CUDA Available After Init\", torch.cuda.is_available())\n",
    "            if torch.cuda.is_available():\n",
    "                print_info(\"GPU Count\", torch.cuda.device_count())\n",
    "                for i in range(torch.cuda.device_count()):\n",
    "                    print_info(f\"GPU {i} Name\", torch.cuda.get_device_name(i))\n",
    "        else:\n",
    "            print(\n",
    "                \"Manual CUDA initialization: Failed (CUDA not found in this PyTorch build)\"\n",
    "            )\n",
    "            print(\"Your PyTorch installation might not include CUDA support.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Manual CUDA initialization error: {e}\")\n",
    "\n",
    "# Final determination about GPU availability\n",
    "print(\"\\nFinal GPU Status:\")\n",
    "if torch.cuda.is_available():\n",
    "    print(\"‚úì CUDA-capable GPU is available and working with PyTorch\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"  - GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "elif platform.system() == \"Windows\" and any(\n",
    "    \"NVIDIA\" in gpu for gpu in locals().get(\"nvidia_output\", [])\n",
    "):\n",
    "    print(\"‚ö† NVIDIA GPU detected by Windows, but not accessible by PyTorch\")\n",
    "    print(\"This typically means one of the following:\")\n",
    "    print(\"  1. Your PyTorch installation was not built with CUDA support\")\n",
    "    print(\"  2. Your NVIDIA drivers need to be updated\")\n",
    "    print(\"  3. Your laptop may be using Optimus/hybrid graphics where the NVIDIA GPU\")\n",
    "    print(\"     is not directly accessible, or has been disabled in power settings\")\n",
    "    nvidia_gpus = [\n",
    "        gpu\n",
    "        for gpu in nvidia_output\n",
    "        if \"NVIDIA\" in gpu\n",
    "        or \"GeForce\" in gpu\n",
    "        or \"Quadro\" in gpu\n",
    "        or \"RTX\" in gpu\n",
    "        or \"GTX\" in gpu\n",
    "    ]\n",
    "    for gpu in nvidia_gpus:\n",
    "        print(f\"  - Detected NVIDIA GPU: {gpu}\")\n",
    "else:\n",
    "    print(\"‚úó No CUDA-capable GPU detected or accessible\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö° GPU PERFORMANCE DASHBOARD ‚ö°\n",
      "\n",
      "üßÆ Linear Algebra Operations\n",
      "‚è±Ô∏è  Testing: Matrix Multiplication (512√ó512)\n",
      "   CPU: 0.005544s ¬± 0.004945s\n",
      "‚è±Ô∏è  Testing: Matrix Multiplication (1024√ó1024)\n",
      "   CPU: 0.029805s ¬± 0.029656s\n",
      "‚è±Ô∏è  Testing: Matrix Multiplication (4096√ó4096)\n",
      "   CPU: 1.558695s ¬± 1.558695s\n",
      "\n",
      "üß† Neural Network Layers\n",
      "‚è±Ô∏è  Testing: Conv2d (64 filters, 3√ó3, 224√ó224 image)\n",
      "   CPU: 0.208079s ¬± 0.207286s\n",
      "‚è±Ô∏è  Testing: ReLU (10M elements)\n",
      "   CPU: 0.061763s ¬± 0.061768s\n",
      "‚è±Ô∏è  Testing: BatchNorm2d (100 images)\n",
      "   CPU: 0.155624s ¬± 0.155604s\n",
      "\n",
      "üìä Data Processing\n",
      "‚è±Ô∏è  Testing: Element-wise Operations (10M elements)\n",
      "   CPU: 0.183403s ¬± 0.183278s\n",
      "\n",
      "‚ùå Cannot create performance dashboard: No CUDA-capable GPU detected or enabled.\n",
      "Run the following command to install PyTorch with CUDA support:\n",
      "uv pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n"
     ]
    }
   ],
   "source": [
    "print(\"‚ö° GPU PERFORMANCE DASHBOARD ‚ö°\")\n",
    "\n",
    "\n",
    "def run_benchmark(name, cpu_code, gpu_code, repeats=5, sub_label=None, short_name=None):\n",
    "    \"\"\"Run a benchmark on both CPU and GPU with elegant timing\"\"\"\n",
    "    print(f\"‚è±Ô∏è  Testing: {name}\")\n",
    "\n",
    "    if short_name is None:\n",
    "        # Extract short name safely without pandas\n",
    "        parts = name.split(\" (\")\n",
    "        short_name = parts[0] if len(parts) > 0 else name\n",
    "\n",
    "    # CPU benchmark\n",
    "    cpu_timer = Timer(\n",
    "        stmt=cpu_code,\n",
    "        globals=globals(),\n",
    "    )\n",
    "    cpu_result = cpu_timer.blocked_autorange(min_run_time=1.0)\n",
    "\n",
    "    # GPU benchmark (if available)\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_timer = Timer(\n",
    "            stmt=gpu_code,\n",
    "            globals=globals(),\n",
    "        )\n",
    "        gpu_result = gpu_timer.blocked_autorange(min_run_time=1.0)\n",
    "\n",
    "        # Calculate speedup\n",
    "        speedup = cpu_result.mean / gpu_result.mean\n",
    "\n",
    "        # Print results with nice formatting\n",
    "        result_str = f\"   CPU: {cpu_result.mean:.6f}s ¬± {cpu_result.median:.6f}s  |  GPU: {gpu_result.mean:.6f}s ¬± {gpu_result.median:.6f}s  |  Speedup: {speedup:.2f}x\"\n",
    "        print(result_str)\n",
    "\n",
    "        return {\n",
    "            \"name\": name,\n",
    "            \"short_name\": short_name,\n",
    "            \"sub_label\": sub_label,\n",
    "            \"cpu_time\": cpu_result.mean,\n",
    "            \"cpu_std\": cpu_result.median,\n",
    "            \"gpu_time\": gpu_result.mean,\n",
    "            \"gpu_std\": gpu_result.median,\n",
    "            \"speedup\": speedup,\n",
    "        }\n",
    "    else:\n",
    "        print(f\"   CPU: {cpu_result.mean:.6f}s ¬± {cpu_result.median:.6f}s\")\n",
    "        return {\n",
    "            \"name\": name,\n",
    "            \"short_name\": short_name,\n",
    "            \"sub_label\": sub_label,\n",
    "            \"cpu_time\": cpu_result.mean,\n",
    "            \"cpu_std\": cpu_result.median,\n",
    "            \"gpu_time\": None,\n",
    "            \"gpu_std\": None,\n",
    "            \"speedup\": None,\n",
    "        }\n",
    "\n",
    "\n",
    "# Create a list to store benchmark results\n",
    "results = []\n",
    "\n",
    "print(\"\\nüßÆ Linear Algebra Operations\")\n",
    "# Test matrix operations at different sizes\n",
    "for size in [512, 1024, 4096]:\n",
    "    # Matrix multiplication\n",
    "    results.append(\n",
    "        run_benchmark(\n",
    "            f\"Matrix Multiplication ({size}√ó{size})\",\n",
    "            f\"torch.matmul(torch.randn({size}, {size}), torch.randn({size}, {size}))\",\n",
    "            f\"torch.matmul(torch.randn({size}, {size}, device='cuda'), torch.randn({size}, {size}, device='cuda')); torch.cuda.synchronize()\",\n",
    "            sub_label=\"Linear Algebra\",\n",
    "            short_name=f\"Matrix Mult {size}√ó{size}\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "print(\"\\nüß† Neural Network Layers\")\n",
    "# Test common neural network operations\n",
    "# Conv2d operation\n",
    "results.append(\n",
    "    run_benchmark(\n",
    "        \"Conv2d (64 filters, 3√ó3, 224√ó224 image)\",\n",
    "        \"torch.nn.functional.conv2d(torch.randn(16, 3, 224, 224), torch.randn(64, 3, 3, 3), padding=1)\",\n",
    "        \"torch.nn.functional.conv2d(torch.randn(16, 3, 224, 224, device='cuda'), torch.randn(64, 3, 3, 3, device='cuda'), padding=1); torch.cuda.synchronize()\",\n",
    "        sub_label=\"Neural Network\",\n",
    "        short_name=\"Conv2d\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# ReLU activation on large tensor\n",
    "results.append(\n",
    "    run_benchmark(\n",
    "        \"ReLU (10M elements)\",\n",
    "        \"torch.nn.functional.relu(torch.randn(10000000))\",\n",
    "        \"torch.nn.functional.relu(torch.randn(10000000, device='cuda')); torch.cuda.synchronize()\",\n",
    "        sub_label=\"Neural Network\",\n",
    "        short_name=\"ReLU\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# BatchNorm on image data\n",
    "results.append(\n",
    "    run_benchmark(\n",
    "        \"BatchNorm2d (100 images)\",\n",
    "        \"bn = torch.nn.BatchNorm2d(64); bn(torch.randn(100, 64, 56, 56))\",\n",
    "        \"bn = torch.nn.BatchNorm2d(64).cuda(); bn(torch.randn(100, 64, 56, 56, device='cuda')); torch.cuda.synchronize()\",\n",
    "        sub_label=\"Neural Network\",\n",
    "        short_name=\"BatchNorm2d\",\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"\\nüìä Data Processing\")\n",
    "# Element-wise operations\n",
    "results.append(\n",
    "    run_benchmark(\n",
    "        \"Element-wise Operations (10M elements)\",\n",
    "        \"a = torch.randn(10000000); b = torch.randn(10000000); c = torch.sin(a) + torch.log(torch.abs(b) + 1)\",\n",
    "        \"a = torch.randn(10000000, device='cuda'); b = torch.randn(10000000, device='cuda'); c = torch.sin(a) + torch.log(torch.abs(b) + 1); torch.cuda.synchronize()\",\n",
    "        sub_label=\"Data Processing\",\n",
    "        short_name=\"Element-wise Ops\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create visualizations without pandas\n",
    "if torch.cuda.is_available() and len(results) > 0:\n",
    "    # Extract data for plotting\n",
    "    names = [r[\"short_name\"] for r in results]\n",
    "    cpu_times = [r[\"cpu_time\"] for r in results]\n",
    "    gpu_times = [r[\"gpu_time\"] for r in results]\n",
    "    speedups = [r[\"speedup\"] for r in results]\n",
    "    sub_labels = [r[\"sub_label\"] for r in results]\n",
    "\n",
    "    # Calculate average metrics\n",
    "    avg_speedup = sum(speedups) / len(speedups)\n",
    "    max_speedup = max(speedups)\n",
    "    min_speedup = min(speedups)\n",
    "\n",
    "    # Find the operations with max and min speedup\n",
    "    max_op = names[speedups.index(max_speedup)]\n",
    "    min_op = names[speedups.index(min_speedup)]\n",
    "\n",
    "    # Get unique categories\n",
    "    unique_labels = list(set(sub_labels))\n",
    "\n",
    "    # Calculate category averages\n",
    "    cat_avg = {}\n",
    "    for label in unique_labels:\n",
    "        label_speedups = [s for i, s in enumerate(speedups) if sub_labels[i] == label]\n",
    "        cat_avg[label] = sum(label_speedups) / len(label_speedups)\n",
    "\n",
    "    # Create a modern dashboard with multiple visualizations\n",
    "    fig = plt.figure(figsize=(14, 10), dpi=100, facecolor=colors[\"background\"])\n",
    "    gs = fig.add_gridspec(2, 2, height_ratios=[2, 1])\n",
    "\n",
    "    # 1. CPU vs GPU Performance comparison (with log scale)\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    bar_width = 0.35\n",
    "    x = np.arange(len(names))\n",
    "\n",
    "    # CPU bars\n",
    "    ax1.bar(\n",
    "        x - bar_width / 2,\n",
    "        cpu_times,\n",
    "        bar_width,\n",
    "        color=colors[\"cpu\"],\n",
    "        label=\"CPU\",\n",
    "        alpha=0.85,\n",
    "        edgecolor=\"white\",\n",
    "        linewidth=0.7,\n",
    "    )\n",
    "\n",
    "    # GPU bars\n",
    "    ax1.bar(\n",
    "        x + bar_width / 2,\n",
    "        gpu_times,\n",
    "        bar_width,\n",
    "        color=colors[\"gpu\"],\n",
    "        label=\"GPU\",\n",
    "        alpha=0.85,\n",
    "        edgecolor=\"white\",\n",
    "        linewidth=0.7,\n",
    "    )\n",
    "\n",
    "    # Format the plot\n",
    "    ax1.set_yscale(\"log\")\n",
    "    ax1.set_ylabel(\"Time (seconds, log scale)\", fontweight=\"bold\")\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(names, rotation=30, ha=\"right\")\n",
    "    ax1.legend(frameon=True)\n",
    "    ax1.set_title(\"CPU vs GPU Execution Time\", fontsize=14, fontweight=\"bold\", pad=15)\n",
    "\n",
    "    # Add value labels\n",
    "    for i, v in enumerate(cpu_times):\n",
    "        ax1.text(\n",
    "            i - bar_width / 2,\n",
    "            v * 1.1,\n",
    "            f\"{v:.3f}s\",\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "            rotation=90,\n",
    "            fontsize=8,\n",
    "            color=colors[\"text\"],\n",
    "        )\n",
    "\n",
    "    for i, v in enumerate(gpu_times):\n",
    "        ax1.text(\n",
    "            i + bar_width / 2,\n",
    "            v * 1.1,\n",
    "            f\"{v:.3f}s\",\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "            rotation=90,\n",
    "            fontsize=8,\n",
    "            color=colors[\"text\"],\n",
    "        )\n",
    "\n",
    "    # 2. Speedup factor chart with gradient color\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "\n",
    "    # Create a gradient color map based on speedup values\n",
    "    norm = plt.Normalize(min(speedups) * 0.8, max(speedups) * 1.2)\n",
    "    sm = plt.cm.ScalarMappable(cmap=\"viridis\", norm=norm)\n",
    "    sm.set_array([])\n",
    "\n",
    "    # Plot horizontal bars with gradient color\n",
    "    bars = ax2.barh(\n",
    "        names,\n",
    "        speedups,\n",
    "        color=sm.to_rgba(speedups),\n",
    "        alpha=0.85,\n",
    "        edgecolor=\"white\",\n",
    "        linewidth=0.7,\n",
    "    )\n",
    "\n",
    "    # Format the plot\n",
    "    ax2.set_xlabel(\"Speedup Factor (√ó)\", fontweight=\"bold\")\n",
    "    ax2.invert_yaxis()  # Invert to match order with first plot\n",
    "    ax2.set_title(\"GPU Speedup over CPU\", fontsize=14, fontweight=\"bold\", pad=15)\n",
    "\n",
    "    # Add value labels\n",
    "    for i, v in enumerate(speedups):\n",
    "        ax2.text(v + 0.5, i, f\"{v:.1f}√ó\", va=\"center\", fontweight=\"bold\")\n",
    "\n",
    "    # Add colorbar\n",
    "    cbar = fig.colorbar(sm, ax=ax2)\n",
    "    cbar.set_label(\"Speedup Factor\", rotation=270, labelpad=20, fontweight=\"bold\")\n",
    "\n",
    "    # 3. Speedup by category\n",
    "    ax3 = fig.add_subplot(gs[1, 0])\n",
    "\n",
    "    # Create horizontal bar chart for categories\n",
    "    cat_names = list(cat_avg.keys())\n",
    "    cat_values = list(cat_avg.values())\n",
    "\n",
    "    # Sort by values\n",
    "    cat_pairs = sorted(zip(cat_names, cat_values), key=lambda x: x[1])\n",
    "    cat_names = [x[0] for x in cat_pairs]\n",
    "    cat_values = [x[1] for x in cat_pairs]\n",
    "\n",
    "    # Plot bars with gradient color\n",
    "    bars = ax3.barh(\n",
    "        cat_names,\n",
    "        cat_values,\n",
    "        color=sns.color_palette(\"viridis\", len(cat_names)),\n",
    "        alpha=0.85,\n",
    "        edgecolor=\"white\",\n",
    "        linewidth=0.7,\n",
    "    )\n",
    "\n",
    "    # Format the plot\n",
    "    ax3.set_xlabel(\"Average Speedup Factor (√ó)\", fontweight=\"bold\")\n",
    "    ax3.set_ylabel(\"\")\n",
    "    ax3.set_title(\"Performance by Workload Type\", fontsize=14, fontweight=\"bold\")\n",
    "\n",
    "    # Add value labels\n",
    "    for i, v in enumerate(cat_values):\n",
    "        ax3.text(v + 0.5, i, f\"{v:.1f}√ó\", va=\"center\", fontweight=\"bold\")\n",
    "\n",
    "    # 4. Summary metrics panel\n",
    "    ax4 = fig.add_subplot(gs[1, 1])\n",
    "    ax4.axis(\"off\")\n",
    "\n",
    "    # Create a fancy summary box\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    summary_text = (\n",
    "        f\"üöÄ Performance Summary\\n\\n\"\n",
    "        f\"GPU: {gpu_name}\\n\"\n",
    "        f\"CUDA: {torch.version.cuda}\\n\\n\"\n",
    "        f\"Average Speedup: {avg_speedup:.1f}√ó\\n\"\n",
    "        f\"Best Speedup: {max_speedup:.1f}√ó ({max_op})\\n\"\n",
    "        f\"Lowest Speedup: {min_speedup:.1f}√ó ({min_op})\\n\\n\"\n",
    "    )\n",
    "\n",
    "    # Performance rating based on average speedup\n",
    "    if avg_speedup > 20:\n",
    "        rating = \"üìà EXCELLENT - Your GPU offers exceptional acceleration!\"\n",
    "    elif avg_speedup > 10:\n",
    "        rating = \"‚úÖ GREAT - Your GPU provides strong acceleration\"\n",
    "    elif avg_speedup > 5:\n",
    "        rating = \"üëç GOOD - Your GPU offers decent acceleration\"\n",
    "    else:\n",
    "        rating = \"üîç MODEST - Consider upgrading for deep learning tasks\"\n",
    "\n",
    "    summary_text += rating\n",
    "\n",
    "    # Add text box with custom styling\n",
    "    props = dict(\n",
    "        boxstyle=\"round,pad=1\", facecolor=\"#f0f0f0\", alpha=0.7, edgecolor=\"#cccccc\"\n",
    "    )\n",
    "    ax4.text(\n",
    "        0.05,\n",
    "        0.95,\n",
    "        summary_text,\n",
    "        transform=ax4.transAxes,\n",
    "        fontsize=11,\n",
    "        verticalalignment=\"top\",\n",
    "        bbox=props,\n",
    "        fontfamily=\"monospace\",\n",
    "    )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(wspace=0.2, hspace=0.3)\n",
    "    plt.show()\n",
    "\n",
    "    # Print a compact final summary\n",
    "    print(f\"\\nüèÜ GPU PERFORMANCE RATING: {rating.split(' - ')[0]}\")\n",
    "    print(f\"GPU provides an average {avg_speedup:.1f}√ó speedup across all benchmarks.\")\n",
    "\n",
    "else:\n",
    "    print(\n",
    "        \"\\n‚ùå Cannot create performance dashboard: No CUDA-capable GPU detected or enabled.\"\n",
    "    )\n",
    "    print(\"Run the following command to install PyTorch with CUDA support:\")\n",
    "    print(\n",
    "        \"uv pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
